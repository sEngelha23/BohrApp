{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChallengeTask Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinweise\n",
    "\n",
    "- __Challenge Modellintegration__:  \n",
    "  In `src/mldog/app/model/challenge_task` finden Sie eine Basisklasse für einen `ChallengeTask`.  \n",
    "  Diese Klasse dient, ähnlich der `Task` Klasse, der Integration Ihres Modells / Ihrer Pipeline in das MLDOG Challenge Framework.  \n",
    "  Im Gegensatz zu der bereits bekannten `Task` Klasse aus MLDOG spezifiziert der `ChallengeTask` eine `predict()`-Methode, die die Daten einer erkannten Bohrung direkt übergeben bekommt und (ausschließlich) das erkannte Material als String zurückgeben soll.  \n",
    "  Implementieren Sie eine neue Challenge Task Klasse für Ihr Team, die von `ChallengeTask` erbt und implementieren Sie die `predict()`-Methode auf Basis Ihres Modells / Ihrer Pipeline.  \n",
    "  Ein Beispiel dazu finden Sie in `src/mldog/app/tasks/example_challenge_task.py`.\n",
    "- Um Ihren eigenen Challenge Task mit diesem Notebook zu testen, müssen Sie die beiden mit TODO gekennzeichneten Stellen in diesem Notebook entsprechend ergänzen:\n",
    "  1. Import Ihres Challenge Tasks\n",
    "  2. Erzeugung einer Instanz Ihres Challenge Tasks\n",
    "\n",
    "  Abgesehen davon sollten Sie keine Änderungen in diesem Notebook vornehmen müssen (sofern Sie die Beispieldaten aus Moodle in das erwartete Verzeichnis entpackt haben).\n",
    "- Denken Sie beim Testen daran, dass alle relativen Pfadangaben relativ zu diesem Notebook sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload external modules (see https://ipython.org/ipython-doc/3/config/extensions/autoreload.html for more information).\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set up system path to include our own \"mldog\" python package.\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# general imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erzeugen der Challenge Task Instanz\n",
    "\n",
    "Importieren Sie hier Ihren eigenen Challenge Task und erzeugen Sie anschließend eine neue Instanz ihres Challenge Tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Importieren Sie hier Ihren eigenen Challenge Task\n",
    "# from mldog.app.tasks.team_X_challenge_task import TeamXChallengeTask\n",
    "from mldog.app.tasks.example_challenge_task import ExampleChallengeTask\n",
    "\n",
    "# TODO: Erzeugen Sie hier eine neue Instanz Ihres Challenge Tasks\n",
    "# task = TeamXChallengeTask()\n",
    "task = ExampleChallengeTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden einer Beispielbohrung\n",
    "\n",
    "Die Daten, die die `predict()`-Methode des Challenge Tasks erhält ist das Ergebnis des `DrillProcedureDetector`s, also ein numpy array mit den Daten einer erkannten Bohrung.\n",
    "\n",
    "Der `DrillProcedureDetector` nutzt die gleichen Parametereinstellungen wie bei der Datenaufzeichnung, also inklusive 0.5 Sekunden vor und nach der eigentlichen Bohrung.\n",
    "\n",
    "Hinweis: Hier wird erwartet, dass Sie die Beispieldaten (aus Moodle) in das übergeordnete `data/raw` Verzeichnis entpackt haben. Sie können hier jedoch auch den Pfad zu einer beliebigen Zeitreihe einer Datenaufzeichnung nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.743164, 17.960386,  0.524393],\n",
       "       [ 1.75293 , 17.960386,  0.304667],\n",
       "       [ 1.762695, 17.960386,  0.524393],\n",
       "       ...,\n",
       "       [ 1.75293 , 17.827257,  0.451151],\n",
       "       [ 1.75293 , 17.827257,  0.377909],\n",
       "       [ 1.75293 , 17.827257,  0.377909]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drill_data = np.loadtxt('../data/raw/2024-10-07_13-10-10/2024_10_07_13_14_55_96000Hz.csv', delimiter=',')\n",
    "drill_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausführen des Challenge Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: holz-span in 3.4397402610000003 seconds.\n"
     ]
    }
   ],
   "source": [
    "# hier wird die run()-Methode des Challenge Tasks aufgerufen, die intern die predict()-Methode aufruft und dabei dessen Laufzeit misst\n",
    "y_pred, runtime = task.run(drill_data)\n",
    "\n",
    "print(f'Predicted: {y_pred} in {runtime} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldog_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
